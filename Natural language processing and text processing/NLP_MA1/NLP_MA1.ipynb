{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824691d8",
   "metadata": {},
   "source": [
    "# Part 1: Mandatory Question\n",
    "### The following two sentences are ambiguous – that is, they each have two (or more) different readings. For each sentence, explain the different readings:\n",
    "1. The girl attacked the boy with the book.\n",
    "\n",
    "Can be read as \"The girl used the book as a weapon to attack the boy\" or \"The girl attacked the boy who's holding a book\"\n",
    "\n",
    "2. We decided to leave on Saturday.\n",
    "\n",
    "Can be read as \"they made the decision on the saturday\" or \"the leaving is scheduled on saturday\"\n",
    "\n",
    "### Are the following sentences ambiguous? If the sentence is ambiguous, explain the different readings. If not, explain why not.\n",
    "3. I saw a man with a briefcase.\n",
    "Both ambigious and not ambigious. Can be read as \"You saw a man carrying a briefcase\" or \"you used a briefcase to see a man\". The second option is grammatically correct but doesn't make any sense. \n",
    "\n",
    "4. I saw the planet with a telescope.\n",
    "Ambigious. Can be read as \"You used a telescope to see a planet\" or \"you saw a planet that has a telescope\"\n",
    "\n",
    "### Try the above sentences (3 and 4) in this parse tree generator: https://huggingface.co/spaces/nanom/syntactic_tree Compare the dependency structures for both of them. Explain the differences in the link to the preposition “with” in 3 vs. 4.\n",
    "\n",
    "Sentence 3: “(I) saw a man with a briefcase.”\n",
    "- **saw** = Root (VBD)\n",
    "- **man** depends on **saw** (the direct object)\n",
    "- **with** depends on **saw** (a PP dependent/adjunct of the verb)\n",
    "- **briefcase** depends on **with** (object of the preposition)\n",
    "- **saw** → with → **briefcase**\n",
    "\n",
    "Sentence 4: “I saw the planet with a telescope.”\n",
    "- **saw** = Root (VBD)\n",
    "- **I** depends on **saw** (subject)\n",
    "- **planet** depends on **saw** (direct object)\n",
    "- **with** depends on **saw**\n",
    "- **telescope** depends on **with**\n",
    "\n",
    "Structurally in the outputs there’s no difference in attachment: in both, **with** attaches to the verb **saw**.\n",
    "\n",
    "Interpretively: the same syntactic attachment has different consequences:\n",
    "\n",
    "- In (3), verb-attachment makes the sentence lean toward the odd reading (“saw using a briefcase”), whereas many humans prefer noun-attachment (“man with a briefcase”).\n",
    "\n",
    "- In (4), verb-attachment produces the very natural instrumental reading (“saw using a telescope”), which is why the parser’s choice looks “right.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca98942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742cd383",
   "metadata": {},
   "source": [
    "# Part 2: Select any one question to answer in this\n",
    "[Linguistic Analysis of a Text Corpus Using spaCy]\n",
    "\n",
    "There is an uploaded text corpus named ”sample.xlsx”. Conduct a linguistic\n",
    "analysis using Part-of-Speech (POS) tagging and Named Entity Recognition\n",
    "(NER) in spaCy library only on the ”SOS Tweet/SOS Message” column.\n",
    "To complete this question you have to do the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/clara.s.nielsen/Desktop/sample.xlsx\"\n",
    "col = \"SOS Tweet / SOS Message\"\n",
    "\n",
    "# ---- Load data  ----\n",
    "df = pd.read_excel(\n",
    "    path, \n",
    "    skiprows= 8, \n",
    "    skipfooter=5, \n",
    "    usecols= [col])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340b6c9",
   "metadata": {},
   "source": [
    "1) Prepare the Corpus: Pre-process the corpus data by removing stopwords, removing extra spaces, converting all text to lowercase, or apply any other text normalization techniques that you think is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a clean list of tweet strings (no NaNs, stripped)\n",
    "tw = (\n",
    "    df[col]\n",
    "    .dropna() \n",
    "    .astype(str) \n",
    "    .str.strip() \n",
    "    .tolist() \n",
    ")\n",
    "\n",
    "print(f\"Loaded: {len(tw)} tweets\")\n",
    "print(tw[:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- spaCy setup (for tokenization) ----\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Stop words: default + extras\n",
    "stop_words = set(STOP_WORDS) | {'helpðÿ', '™', '\\x8fðÿ', '™', '\\x8f', 'ðÿ'}\n",
    "stop_words.update({\"rt\", \"via\", \"amp\"})\n",
    "\n",
    "\n",
    "# ---- Tokenize each tweet ----\n",
    "tokenized_tweets = [\n",
    "    [tok.text for tok in doc]\n",
    "    for doc in nlp.pipe(tw, batch_size=200)\n",
    "]\n",
    "print(tokenized_tweets[:2])\n",
    "\n",
    "\n",
    "# ---- Clean tokens per tweet ----\n",
    "cleaned_tweets = [\n",
    "    [\n",
    "        tok.text.lower().strip(\"-\")\n",
    "        for tok in doc\n",
    "        if not tok.is_space \n",
    "        and not tok.is_punct \n",
    "        and tok.text.lower() not in stop_words \n",
    "        if tok.is_alpha or tok.text.rstrip(\"-\").isalpha()\n",
    "        #and tok.text.isalpha()\n",
    "    ]\n",
    "    for doc in nlp.pipe(tw, batch_size=200)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(cleaned_tweets[83:85])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
