{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824691d8",
   "metadata": {},
   "source": [
    "# Part 1: Mandatory Question\n",
    "### The following two sentences are ambiguous – that is, they each have two (or more) different readings. For each sentence, explain the different readings:\n",
    "1. The girl attacked the boy with the book.\n",
    "\n",
    "Can be read as \"The girl used the book as a weapon to attack the boy\" or \"The girl attacked the boy who's holding a book\"\n",
    "\n",
    "2. We decided to leave on Saturday.\n",
    "\n",
    "Can be read as \"they made the decision on the saturday\" or \"the leaving is scheduled on saturday\"\n",
    "\n",
    "### Are the following sentences ambiguous? If the sentence is ambiguous, explain the different readings. If not, explain why not.\n",
    "3. I saw a man with a briefcase.\n",
    "Both ambigious and not ambigious. Can be read as \"You saw a man carrying a briefcase\" or \"you used a briefcase to see a man\". The second option is grammatically correct but doesn't make any sense. \n",
    "\n",
    "4. I saw the planet with a telescope.\n",
    "Ambigious. Can be read as \"You used a telescope to see a planet\" or \"you saw a planet that has a telescope\"\n",
    "\n",
    "### Try the above sentences (3 and 4) in this parse tree generator: https://huggingface.co/spaces/nanom/syntactic_tree Compare the dependency structures for both of them. Explain the differences in the link to the preposition “with” in 3 vs. 4.\n",
    "\n",
    "Sentence 3: “(I) saw a man with a briefcase.”\n",
    "- **saw** = Root (VBD)\n",
    "- **man** depends on **saw** (the direct object)\n",
    "- **with** depends on **saw** (a PP dependent/adjunct of the verb)\n",
    "- **briefcase** depends on **with** (object of the preposition)\n",
    "- **saw** → with → **briefcase**\n",
    "\n",
    "Sentence 4: “I saw the planet with a telescope.”\n",
    "- **saw** = Root (VBD)\n",
    "- **I** depends on **saw** (subject)\n",
    "- **planet** depends on **saw** (direct object)\n",
    "- **with** depends on **saw**\n",
    "- **telescope** depends on **with**\n",
    "\n",
    "Structurally in the outputs there’s no difference in attachment: in both, **with** attaches to the verb **saw**.\n",
    "\n",
    "Interpretively: the same syntactic attachment has different consequences:\n",
    "\n",
    "- In (3), verb-attachment makes the sentence lean toward the odd reading (“saw using a briefcase”), whereas many humans prefer noun-attachment (“man with a briefcase”).\n",
    "\n",
    "- In (4), verb-attachment produces the very natural instrumental reading (“saw using a telescope”), which is why the parser’s choice looks “right.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcd55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd \n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca98942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Using cached prettytable-3.17.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: wcwidth in /Users/axellidvall/Library/CloudStorage/OneDrive-Privat/CBS/Semester 2/Group repository/.venv/lib/python3.13/site-packages (from prettytable) (0.6.0)\n",
      "Using cached prettytable-3.17.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# downloading the English model for spaCy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#%pip install prettytable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742cd383",
   "metadata": {},
   "source": [
    "# Part 2: Select any one question to answer in this\n",
    "[Linguistic Analysis of a Text Corpus Using spaCy]\n",
    "\n",
    "There is an uploaded text corpus named ”sample.xlsx”. Conduct a linguistic\n",
    "analysis using Part-of-Speech (POS) tagging and Named Entity Recognition\n",
    "(NER) in spaCy library only on the ”SOS Tweet/SOS Message” column.\n",
    "To complete this question you have to do the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5485a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOS Tweet / SOS Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my relative B************n who is from bagmara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Redacted Mention] [Redacted Mention] we need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#SOSDehradun #Plasma4Covid&nbsp;&nbsp;Urgent Plasma requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#SOS.#SOS : Oxygen bed required&nbsp;&nbsp;Location: Coi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Name: Y****************m&nbsp;&nbsp;Age: 62&nbsp;&nbsp;Location: H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SOS Tweet / SOS Message\n",
       "0  my relative B************n who is from bagmara...\n",
       "1  [Redacted Mention] [Redacted Mention] we need ...\n",
       "2  #SOSDehradun #Plasma4Covid  Urgent Plasma requ...\n",
       "3  #SOS.#SOS : Oxygen bed required  Location: Coi...\n",
       "4  Name: Y****************m  Age: 62  Location: H..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the Excel file and stored the specified column name in a variable\n",
    "path = \"sample.xlsx\"\n",
    "col = \"SOS Tweet / SOS Message\"\n",
    "\n",
    "# 1) Load the Excel file \"sample.xlsx\"\n",
    "df = pd.read_excel(path, skiprows= 8, skipfooter=5, usecols=[col])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340b6c9",
   "metadata": {},
   "source": [
    "1) Prepare the Corpus: Pre-process the corpus data by removing stopwords, removing extra spaces, converting all text to lowercase, or apply any other text normalization techniques that you think is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2726e054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 100 tweets\n",
      "['my relative B************n who is from bagmara jharkhand is tested positive for corona and is admitted in recovery nourshing home bardmaan West Bengal.He is not getting sufficient treatment and his health is getting worse. We need an urgent ventilation bed. HELP [Redacted Mention]']\n"
     ]
    }
   ],
   "source": [
    "# 2) Nested list with each tweet as a separate list\n",
    "tw = (\n",
    "    df[col]\n",
    "    .dropna() \n",
    "    .astype(str) \n",
    "    .str.strip() \n",
    "    .tolist() \n",
    ")\n",
    "\n",
    "# Overview of the loaded tweets\n",
    "print(f\"Loaded: {len(tw)} tweets\")\n",
    "print(tw[:1]) #view the first tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c2718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'relative', 'B************n', 'who', 'is', 'from', 'bagmara', 'jharkhand', 'is', 'tested', 'positive', 'for', 'corona', 'and', 'is', 'admitted', 'in', 'recovery', 'nourshing', 'home', 'bardmaan', 'West', 'Bengal', '.', 'He', 'is', 'not', 'getting', 'sufficient', 'treatment', 'and', 'his', 'health', 'is', 'getting', 'worse', '.', 'We', 'need', 'an', 'urgent', 'ventilation', 'bed', '.', 'HELP', '[', 'Redacted', 'Mention', ']'], ['[', 'Redacted', 'Mention', ']', '[', 'Redacted', 'Mention', ']', 'we', 'need', 'urgently', 'oxygen', 'sylinder', 'in', 'Nadiad', 'for', 'our', 'grand', 'fother', '..', 'please', 'help', 'us', '..']]\n",
      "[['relative', 'b************n', 'bagmara', 'jharkhand', 'tested', 'positive', 'corona', 'admitted', 'recovery', 'nourshing', 'home', 'bardmaan', 'west', 'bengal', 'getting', 'sufficient', 'treatment', 'health', 'getting', 'worse', 'need', 'urgent', 'ventilation', 'bed', 'help', 'redacted', 'mention'], ['redacted', 'mention', 'redacted', 'mention', 'need', 'urgently', 'oxygen', 'sylinder', 'nadiad', 'grand', 'fother', 'help']]\n"
     ]
    }
   ],
   "source": [
    "# 3) setup spaCy for tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# As the tweets anonymizes names with a pattern like \"A***B\", we can add a custom entity ruler to recognize these as PERSON entities.\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "patterns = [\n",
    "    {\n",
    "        \"label\": \"PERSON\",\n",
    "        \"pattern\": [{\"TEXT\": {\"REGEX\": r\"^[A-Za-z]\\*+[A-Za-z]$\"}}]\n",
    "    }\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Set up the stop words: default + extras\n",
    "stop_words = set(STOP_WORDS) | {'helpðÿ', '™', '\\x8fðÿ', '\\x8f', 'ðÿ', '+', '@', \"#\", \"*\", 'a+'}\n",
    "#stop_words.update({'helpðÿ', '™', '\\x8fðÿ', '\\x8f', 'ðÿ', '+', '@', \"#\", \"*\", 'a+'})\n",
    "\n",
    "\n",
    "# tokenize the tweets using spaCy's nlp.pipe with batch processing for efficiency\n",
    "tokenized_tweets = [\n",
    "    [tok.text for tok in doc]\n",
    "    for doc in nlp.pipe(tw, batch_size=200)\n",
    "]\n",
    "print(tokenized_tweets[:2])\n",
    "\n",
    "\n",
    "# clean the tokenized tweets by removing stop words, punctuation, and whitespace, and converting to lowercase\n",
    "cleaned_tweets = [\n",
    "    [\n",
    "        tok.text.lower().strip(\"-\") # remove leading/trailing \"-\" but keep internal hyphens\n",
    "        for tok in doc\n",
    "        if not tok.is_space \n",
    "        and not tok.is_punct \n",
    "        and tok.text.lower() not in stop_words \n",
    "    ]\n",
    "    for doc in nlp.pipe(tw, batch_size=200)\n",
    "]\n",
    "\n",
    "print(cleaned_tweets[:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bad0c7",
   "metadata": {},
   "source": [
    "2. POS Tagging and NER: Apply POS tagging to the entire corpus to\n",
    "analyze the distribution of different parts of speech. Use NER to identify\n",
    "and categorize named entities within the text such as name, date, locations\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5dd7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|  POS  | Count |\n",
      "+-------+-------+\n",
      "|  NOUN |  1092 |\n",
      "|  VERB |  499  |\n",
      "| PROPN |  469  |\n",
      "|  NUM  |  201  |\n",
      "|  ADJ  |  157  |\n",
      "|  ADV  |   34  |\n",
      "|  AUX  |   20  |\n",
      "|  ADP  |   16  |\n",
      "|  INTJ |   13  |\n",
      "| PUNCT |   9   |\n",
      "|   X   |   8   |\n",
      "|  PRON |   5   |\n",
      "| CCONJ |   4   |\n",
      "|  DET  |   3   |\n",
      "|  PART |   2   |\n",
      "|  SYM  |   1   |\n",
      "| SCONJ |   1   |\n",
      "+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "cleaned_texts = [\" \".join(tokens) for tokens in cleaned_tweets]\n",
    "\n",
    "pos_counts = Counter(\n",
    "    tok.pos_ \n",
    "    for doc in nlp.pipe(cleaned_texts, batch_size=200) \n",
    "    for tok in doc\n",
    ")\n",
    "\n",
    "#the POS counts into a pretty table\n",
    "pos_tab = PrettyTable([\"POS\", \"Count\"])\n",
    "for pos,count in pos_counts.most_common():\n",
    "    pos_tab.add_row([pos,count])\n",
    "\n",
    "print(pos_tab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ea7b9",
   "metadata": {},
   "source": [
    "# Question 2.3 – Analysis\n",
    "\n",
    "Nouns dominate, which shows the messages focus on concrete entities like resources, people, and medical needs rather than storytelling. Verbs are the next most common, reflecting that many lines are framed as direct actions or requests (for example “need,” “require,” “contact,” “help”). Proper nouns are also frequent, meaning the texts often include specific places, hospitals, and names to make the request actionable. The relatively high number of numbers indicates lots of critical details like phone numbers, ages, and oxygen/SpO2 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b383ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|   NER    | Count |\n",
      "+----------+-------+\n",
      "| CARDINAL |  116  |\n",
      "|  PERSON  |  107  |\n",
      "|   DATE   |   49  |\n",
      "|   ORG    |   25  |\n",
      "|   GPE    |   8   |\n",
      "| QUANTITY |   4   |\n",
      "|  EVENT   |   1   |\n",
      "| PRODUCT  |   1   |\n",
      "| ORDINAL  |   1   |\n",
      "|   NORP   |   1   |\n",
      "|   TIME   |   1   |\n",
      "|   LAW    |   1   |\n",
      "+----------+-------+\n"
     ]
    }
   ],
   "source": [
    "# NER counts\n",
    "\n",
    "entity_counts = Counter(\n",
    "    ent.label_ \n",
    "    for doc in nlp.pipe(cleaned_texts, batch_size=200) \n",
    "    for ent in doc.ents\n",
    ")\n",
    "\n",
    "ner_tab = PrettyTable([\"NER\", \"Count\"])\n",
    "for ner,count in entity_counts.most_common():\n",
    "    ner_tab.add_row([ner,count])\n",
    "\n",
    "print(ner_tab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
